{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_TklzLYAcDe"
   },
   "source": [
    "# Import & Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: tensorflow in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow opencv-python matplotlib numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a6oWnah-AcDg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JptjJAtLAcDh"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.utils import to_categorical\n",
    "from plotly import tools, subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot  # TensorFlow Model Optimization toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset organized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Organizing images based on class\n",
    "\n",
    "# Define the dataset path\n",
    "dataset_path = \"E:/Lian/S2/Datasets/kannada_characters/img\"\n",
    "\n",
    "# Create a destination folder to organize images\n",
    "organized_dataset_directory = \"E:/Lian/S2/Datasets/kannada_characters/organized\"\n",
    "if not os.path.exists(organized_dataset_directory):\n",
    "    os.makedirs(organized_dataset_directory)\n",
    "\n",
    "# Iterate over the files in the dataset\n",
    "for file_name in os.listdir(dataset_path):\n",
    "    if file_name.endswith(\".png\"):\n",
    "        # Extract the class/label from the file name (e.g., img001)\n",
    "        label = file_name.split('-')[0]\n",
    "        \n",
    "        # Create a subfolder for each label if it doesn't exist\n",
    "        label_dir = os.path.join(organized_dataset_directory, label)\n",
    "        if not os.path.exists(label_dir):\n",
    "            os.makedirs(label_dir)\n",
    "        \n",
    "        # Move the file into the appropriate label directory\n",
    "        source_file = os.path.join(dataset_path, file_name)\n",
    "        destination_file = os.path.join(label_dir, file_name)\n",
    "        shutil.move(source_file, destination_file)\n",
    "\n",
    "print(\"Dataset organized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 657\n"
     ]
    }
   ],
   "source": [
    "num_classes = len([name for name in os.listdir(organized_dataset_directory) if os.path.isdir(os.path.join(organized_dataset_directory, name))])\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16425 files belonging to 657 classes.\n",
      "Using 14783 files for training.\n",
      "Using 1642 files for validation.\n"
     ]
    }
   ],
   "source": [
    "# Load images using image_dataset_from_directory\n",
    "dataset_path = \"E:/Lian/S2/Datasets/kannada_characters/organized\"\n",
    "\n",
    "# Parameters\n",
    "image_size = (64, 64)  # Image dimensions\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Create train and test datasets\n",
    "dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    image_size=image_size,\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int',\n",
    "    validation_split=0.1,  # Split data into 90% training, 10% validation\n",
    "    subset='both',\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Unpack train and validation datasets\n",
    "train_dataset, test_dataset = dataset\n",
    "\n",
    "# Normalize pixel values\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<BatchDataset element_spec=(TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>,\n",
       " <BatchDataset element_spec=(TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().squeeze(), cmap='gray')\n",
    "        plt.title(f\"Label: {labels[i].numpy()}\")\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.InputLayer(input_shape=(64, 64, 1)),\n",
    "#     tf.keras.layers.Conv2D(8, (4, 4), activation='relu', padding='same'),\n",
    "#     tf.keras.layers.MaxPooling2D((8, 8), padding='same'),\n",
    "#     tf.keras.layers.Conv2D(16, (2, 2), activation='relu', padding='same'),\n",
    "#     tf.keras.layers.MaxPooling2D((4, 4), padding='same'),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.InputLayer(input_shape=(64, 64, 1)),\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(128, activation='relu'),\n",
    "#     tf.keras.layers.Dropout(0.5),\n",
    "#     tf.keras.layers.Dense(num_classes, activation='softmax')  # Change to softmax for multiclass classification\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "462/462 [==============================] - 116s 247ms/step - loss: 6.4907 - accuracy: 9.4703e-04 - val_loss: 6.4916 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "442/462 [===========================>..] - ETA: 3s - loss: 6.4892 - accuracy: 0.0011"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(num_classes))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "462/462 [==============================] - ETA: 0s - loss: 6.4326 - accuracy: 0.0019"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(train_dataset, validation_data=test_dataset, epochs=10)\n",
    "\n",
    "# # Evaluate the model\n",
    "# loss, accuracy = model.evaluate(test_dataset)\n",
    "# print(f\"Test accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNRpUtiDAcDq"
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPjXoCCtncKR"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxuPZ2T1nejf"
   },
   "outputs": [],
   "source": [
    "def plot_images_sample(dataset, num_samples=25):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Get a batch from the dataset\n",
    "    for images, labels in dataset.take(1):  # Take one batch\n",
    "        rand_indices = np.random.randint(images.shape[0], size=num_samples)  # Random indices from the batch\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(5, 5, i + 1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "\n",
    "            index = rand_indices[i]\n",
    "            plt.imshow(np.squeeze(images[index]), cmap=plt.cm.binary)\n",
    "            plt.xlabel(labels[index].numpy())  # Convert to numpy for display\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "plot_images_sample(train_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "K6IYsUj_AcDr",
    "outputId": "91161044-c4f6-483b-84ee-e846869837e1"
   },
   "outputs": [],
   "source": [
    "# Draw plot for CNN training\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.ylim([0.5, 1])\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vh5jqDsnAcDr",
    "outputId": "a151681f-e942-42c2-ec3f-be2dc9d922f2"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SATT44wdAcDs"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_prediction(dataset, predictions, num_samples=25):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "\n",
    "    # Get a batch from the dataset\n",
    "    for images, labels in dataset.take(1):  # Take one batch\n",
    "        rand_indices = np.random.randint(images.shape[0], size=num_samples)  # Random indices from the batch\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            plt.subplot(5, 5, i + 1)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            plt.grid(False)\n",
    "\n",
    "            index = rand_indices[i]\n",
    "            plt.imshow(np.squeeze(images[index]), cmap=plt.cm.binary)\n",
    "            plt.xlabel(f\"Pred: {predictions[index].item()} -> True: {labels[index].numpy()}\")  # Show both predicted and true labels\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "rcvyrtgeAcDs",
    "outputId": "cd9fb1e2-50e8-4ff9-e652-618b27cd24ba"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "plot_images_prediction(test_dataset, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KyF6_k1AcDx"
   },
   "source": [
    "# Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIgUicAlAcDx",
    "outputId": "dc6b9409-19f9-4472-f2d1-117dfbbc5728"
   },
   "outputs": [],
   "source": [
    "# model_save_path = 'cnn_model.h5'\n",
    "\n",
    "model_save_path = 'kannada_model_1.keras'\n",
    "\n",
    "model.save(model_save_path)\n",
    "\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "\n",
    "print(f\"Model size: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UNyLpBrAcDx"
   },
   "source": [
    "# Define the model and apply the sparsity API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDyZ7SdTAcD6",
    "outputId": "43b7aef7-d1c3-47ca-e912-b9d0c7b2fbe0"
   },
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "pruning_params = {\n",
    "    'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.1, begin_step=0, frequency=100)\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "pruned_model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "pruned_model.compile(\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P-bMkwxAcD7"
   },
   "source": [
    "# Pruning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNLmQDdxAcD7",
    "outputId": "4845c0dd-ed79-4985-d12a-6f2216830b7d"
   },
   "outputs": [],
   "source": [
    "pruned_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA5jCS79I3hu",
    "outputId": "5b178c0b-4d71-4322-b00c-897c061d6c78"
   },
   "outputs": [],
   "source": [
    "model_save_path = 'kannada_model_pruned_1.keras'\n",
    "pruned_model.save(model_save_path)\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "print(f\"Model size after pruning: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2DYobRsR6J7",
    "outputId": "4d799a5a-1a0f-46db-b2c6-9d533dd2ea1a"
   },
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "model_save_path = 'kannada_model_pruned_1.tflite'\n",
    "with open(model_save_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "print(f\"Model size after pruning + convert to tf lite: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkYL9ENMVemN"
   },
   "source": [
    "# Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIYuob3MVhSF"
   },
   "outputs": [],
   "source": [
    "# Strip the pruning wrappers to finalize the pruned model\n",
    "final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Convert to TensorFlow Lite model with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "\n",
    "# Set quantization parameters\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Default optimization includes quantization\n",
    "\n",
    "# Provide a representative dataset for better accuracy in quantization\n",
    "def representative_dataset():\n",
    "    for data in X_test.take(100):  # use a small sample of your data <<< what if we change this to 1000? to all data? 70rb? how?\n",
    "        yield [tf.dtypes.cast(data, tf.float32)] # can we change this to integer?\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_types = [tf.float16]  # This can be int8 as well, hm... << nope, error\n",
    "\n",
    "# Convert the model\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "quantized_model = 'quantized_pruned_model_kannada_1.tflite'\n",
    "with open(quantized_model, 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# To load and use the TFLite model later:\n",
    "interpreter = tf.lite.Interpreter(model_path=quantized_model)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfaA3NBHrJHh"
   },
   "outputs": [],
   "source": [
    "final_model.compile(\n",
    "  loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6qoUcObq2w-",
    "outputId": "645c2ab6-f34f-4328-8066-f18654024c84"
   },
   "outputs": [],
   "source": [
    "final_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=10,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8x5jEvJ2Vrcz",
    "outputId": "7adfafec-52c5-4e59-c19e-326defd52498"
   },
   "outputs": [],
   "source": [
    "model_size = os.path.getsize(quantized_model)\n",
    "\n",
    "print(f\"Model size after quantization: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK-s0GOQY-zQ"
   },
   "source": [
    "# Optimized Model with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAu3DrpuZAG0",
    "outputId": "bb8d18db-5f92-4817-ffad-06b366e99974"
   },
   "outputs": [],
   "source": [
    "# Custom loss function for Knowledge Distillation\n",
    "def distillation_loss(y_true, y_pred, teacher_logits, temperature=5.0, alpha=0.5):\n",
    "    student_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    distillation_loss = tf.keras.losses.KLDivergence()(\n",
    "        tf.nn.softmax(teacher_logits / temperature),\n",
    "        tf.nn.softmax(y_pred / temperature)\n",
    "    )\n",
    "    return alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "\n",
    "# Train student model with teacher model's logits\n",
    "def train_student_model(student_model, teacher_model, X_train, y_train):\n",
    "    teacher_logits = teacher_model.predict(X_train)\n",
    "\n",
    "    # Custom training loop\n",
    "    start_kd_time = time.time()    \n",
    "    for epoch in range(10):\n",
    "        print(f\"Epoch {epoch + 1}/10\")\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for i in range(0, len(X_train), 32):\n",
    "            X_batch = X_train[i:i+32]\n",
    "            y_batch = y_train[i:i+32]\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = student_model(X_batch, training=True)\n",
    "                loss = distillation_loss(y_batch, y_pred, teacher_logits[i:i+32])\n",
    "            grads = tape.gradient(loss, student_model.trainable_variables)\n",
    "            student_model.optimizer.apply_gradients(zip(grads, student_model.trainable_variables))\n",
    "\n",
    "            # Track loss and accuracy\n",
    "            epoch_loss += loss.numpy().sum() * len(y_batch)  # Ensure loss is a scalar\n",
    "            correct_predictions += np.sum(np.argmax(y_pred.numpy(), axis=-1) == np.argmax(y_batch, axis=-1))\n",
    "            total_predictions += len(y_batch)\n",
    "\n",
    "        average_loss = epoch_loss / total_predictions\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        print(f\"Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_kd_time \n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Train the student model\n",
    "student_model = model\n",
    "train_student_model(model, final_model, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VJJWrMkTI3I",
    "outputId": "4f7afe7a-34c2-445d-8a04-7c667d49a209"
   },
   "outputs": [],
   "source": [
    "# Evaluate student model\n",
    "\n",
    "student_loss, student_accuracy = student_model.evaluate(X_test, Y_test)\n",
    "print(f\"Student Model Accuracy: {student_accuracy}, Loss: {student_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNVOGzHFJGAF",
    "outputId": "74e11e04-b915-4687-9195-ea53908c16ac"
   },
   "outputs": [],
   "source": [
    "model_save_path = 'student_model_1_kannada.keras'\n",
    "student_model.save(model_save_path)\n",
    "\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "\n",
    "print(f\"Model size: {model_size / 1024:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1175444,
     "sourceId": 1972687,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "digit-recognition",
   "language": "python",
   "name": "digit-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
