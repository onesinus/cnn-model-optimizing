{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipkmrMKH94me"
   },
   "source": [
    "# model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xB6hrGdc8PlJ",
    "outputId": "960e1862-a144-463a-d0c6-6806ad3b8d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                102464    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,930\n",
      "Trainable params: 121,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, -1).astype(\"float32\") / 255.0\n",
    "X_test = np.expand_dims(X_test, -1).astype(\"float32\") / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Define original CNN model\n",
    "def create_original_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        keras.layers.MaxPooling2D((2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(64, activation='relu'),\n",
    "        keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Compile and train original model\n",
    "original_model = create_original_model()\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QQ46krPhCo2u",
    "outputId": "d0d72537-d88d-401b-dc0d-56ab5fe62534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.1640 - accuracy: 0.9508 - val_loss: 0.0677 - val_accuracy: 0.9800\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0519 - accuracy: 0.9844 - val_loss: 0.0515 - val_accuracy: 0.9854\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0362 - accuracy: 0.9878 - val_loss: 0.0473 - val_accuracy: 0.9860\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 0.0410 - val_accuracy: 0.9879\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 25s 17ms/step - loss: 0.0208 - accuracy: 0.9931 - val_loss: 0.0386 - val_accuracy: 0.9888\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0447 - val_accuracy: 0.9890\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 24s 16ms/step - loss: 0.0129 - accuracy: 0.9958 - val_loss: 0.0396 - val_accuracy: 0.9908\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0099 - accuracy: 0.9965 - val_loss: 0.0512 - val_accuracy: 0.9883\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0086 - accuracy: 0.9969 - val_loss: 0.0470 - val_accuracy: 0.9892\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0524 - val_accuracy: 0.9907\n",
      "Total time: 225.46 seconds\n"
     ]
    }
   ],
   "source": [
    "original_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "start_original_time = time.time()\n",
    "original_model.fit(X_train, y_train, epochs=10, validation_split=0.2, batch_size=32)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_original_time \n",
    "print(f\"Total time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0380 - accuracy: 0.9915\n",
      "Original Model Accuracy: 0.9915000200271606, Loss: 0.03797163441777229\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original model\n",
    "original_loss, original_accuracy = original_model.evaluate(X_test, y_test)\n",
    "print(f\"Original Model Accuracy: {original_accuracy}, Loss: {original_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkMQdy5i-Ll5",
    "outputId": "3e24a048-7479-429d-d754-69f0f98e088b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1473.87 KB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# model_save_path = 'cnn_model.h5'\n",
    "model_save_path = 'model2.keras'\n",
    "original_model.save(model_save_path)\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "print(f\"Model size: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRmtfg0PB1sH"
   },
   "source": [
    "# Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZe-b1_QCCJb",
    "outputId": "0861cd33-d760-4947-fbe9-582ded7ba135"
   },
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot  # TensorFlow Model Optimization toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dkI4XhcKB2y1",
    "outputId": "a70acfba-95b8-4f0f-a87d-96d8fcc4b74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d   (None, 26, 26, 32)       610       \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 13, 13, 32)       1         \n",
      " ling2d (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 11, 11, 64)       36930     \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 5, 5, 64)         1         \n",
      " ling2d_1 (PruneLowMagnitude                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 1600)             1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 64)               204866    \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_1  (None, 10)               1292      \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 243,701\n",
      "Trainable params: 121,930\n",
      "Non-trainable params: 121,771\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "pruning_params = {\n",
    "      # 'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100)\n",
    "      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.1, begin_step=0, frequency=100)\n",
    "  }\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "pruned_model = prune_low_magnitude(original_model, **pruning_params)\n",
    "\n",
    "# Use smaller learning rate for fine-tuning\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "pruned_model.compile(\n",
    "  loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_2jei0CHB97E",
    "outputId": "a3dce854-c907-40ed-f795-edf400a42c8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\keras\\backend.py:5531: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 26s 15ms/step - loss: 0.0063 - accuracy: 0.9978 - val_loss: 0.0631 - val_accuracy: 0.9875\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0055 - accuracy: 0.9980 - val_loss: 0.0552 - val_accuracy: 0.9895\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0522 - val_accuracy: 0.9907\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0054 - accuracy: 0.9984 - val_loss: 0.0501 - val_accuracy: 0.9912\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0515 - val_accuracy: 0.9910\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0771 - val_accuracy: 0.9873\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0627 - val_accuracy: 0.9885\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0622 - val_accuracy: 0.9901\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0559 - val_accuracy: 0.9910\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 22s 15ms/step - loss: 0.0027 - accuracy: 0.9989 - val_loss: 0.0692 - val_accuracy: 0.9896\n",
      "Total time: 227.09 seconds\n"
     ]
    }
   ],
   "source": [
    "start_pruning_time = time.time()\n",
    "pruned_model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  epochs=10,\n",
    "  validation_split=0.2,\n",
    "  callbacks=callbacks)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_pruning_time \n",
    "print(f\"Total time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0544 - accuracy: 0.9902\n",
      "Test accuracy: 0.9901999831199646\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "pruned_test_loss, pruned_test_acc = pruned_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {pruned_test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWr2HCqIB39h",
    "outputId": "2428777e-64a1-4d3e-cff1-983910ce4bc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning: 1962.79 KB\n"
     ]
    }
   ],
   "source": [
    "model_save_path = 'cnn_pruned_model2.keras'\n",
    "pruned_model.save(model_save_path)\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "print(f\"Model size after pruning: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uf6Os_KfMQ9y",
    "outputId": "7b94b1ef-7a66-4b76-8304-d47883a667b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmp_7yohtox\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmp_7yohtox\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning + convert to tf lite: 479.61 KB\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "model_save_path = 'pruned_model2.tflite'\n",
    "with open('pruned_model2.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "print(f\"Model size after pruning + convert to tf lite: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjpIwk9-B3GS"
   },
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6UT9TGGTIcNa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmpyp4e977f\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmpyp4e977f\\assets\n"
     ]
    }
   ],
   "source": [
    "# Strip the pruning wrappers to finalize the pruned model\n",
    "final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Convert to TensorFlow Lite model with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "\n",
    "# Set quantization parameters\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Default optimization includes quantization\n",
    "\n",
    "# Optionally, if you have representative data, you can enable full integer quantization:\n",
    "# Provide a representative dataset for better accuracy in quantization\n",
    "def representative_dataset():\n",
    "    for data in X_test.take(100):  # use a small sample of your data <<< what if we change this to 1000? to all data? 70rb? how?\n",
    "        yield [tf.dtypes.cast(data, tf.float32)] # can we change this to integer?\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_types = [tf.float16]  # This can be int8 as well, hm... << nope, error\n",
    "\n",
    "# Convert the model\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model to a file\n",
    "with open('quantized_pruned_model2.tflite', 'wb') as f:\n",
    "    f.write(quantized_tflite_model)\n",
    "\n",
    "# To load and use the TFLite model later:\n",
    "interpreter = tf.lite.Interpreter(model_path='quantized_pruned_model2.tflite')\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1600)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                102464    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 121,930\n",
      "Trainable params: 121,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WXVXOC1-Ipk7"
   },
   "outputs": [],
   "source": [
    "final_model.compile(\n",
    "  loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  optimizer=opt,\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXR8QECMIten",
    "outputId": "2bb0137d-b75b-475f-b23e-63725999ced5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\keras\\backend.py:5531: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 22s 14ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 0.0584 - val_accuracy: 0.9909\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0648 - val_accuracy: 0.9898\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 20s 14ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0649 - val_accuracy: 0.9912\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0671 - val_accuracy: 0.9895\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0641 - val_accuracy: 0.9903\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0776 - val_accuracy: 0.9888\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0611 - val_accuracy: 0.9912\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 20s 14ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0642 - val_accuracy: 0.9912\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 20s 14ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0702 - val_accuracy: 0.9903\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 21s 14ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0688 - val_accuracy: 0.9911\n",
      "Total time: 204.23 seconds\n"
     ]
    }
   ],
   "source": [
    "start_quantization_time = time.time()\n",
    "final_model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  epochs=10,\n",
    "  validation_split=0.2,\n",
    "  callbacks=callbacks)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_quantization_time \n",
    "print(f\"Total time: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 7ms/step - loss: 0.0606 - accuracy: 0.9913\n",
      "Test accuracy: 0.9912999868392944\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "quantization_test_loss, quantization_test_acc = final_model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f\"Test accuracy: {quantization_test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zaTURCDIvgP",
    "outputId": "7683a35f-67b8-44f3-a31b-bd94c5d1d488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after quantization: 242.58 KB\n"
     ]
    }
   ],
   "source": [
    "model_size = os.path.getsize('quantized_pruned_model2.tflite')\n",
    "print(f\"Model size after quantization: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii8OHpBm96h0"
   },
   "source": [
    "# Optimized Model with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WmX6HqMu98rB",
    "outputId": "ed225d3d-c336-4ab7-df58-f473a16bd6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 10s 5ms/step\n",
      "Epoch 1/10\n",
      "Loss: 0.0557, Accuracy: 0.9991\n",
      "Epoch 2/10\n",
      "Loss: 0.0498, Accuracy: 0.9990\n",
      "Epoch 3/10\n",
      "Loss: 0.0327, Accuracy: 0.9995\n",
      "Epoch 4/10\n",
      "Loss: 0.0487, Accuracy: 0.9991\n",
      "Epoch 5/10\n",
      "Loss: 0.0358, Accuracy: 0.9994\n",
      "Epoch 6/10\n",
      "Loss: 0.0363, Accuracy: 0.9994\n",
      "Epoch 7/10\n",
      "Loss: 0.0544, Accuracy: 0.9991\n",
      "Epoch 8/10\n",
      "Loss: 0.0335, Accuracy: 0.9993\n",
      "Epoch 9/10\n",
      "Loss: 0.0493, Accuracy: 0.9992\n",
      "Epoch 10/10\n",
      "Loss: 0.0261, Accuracy: 0.9995\n",
      "Total time: 722.30 seconds\n"
     ]
    }
   ],
   "source": [
    "# Custom loss function for Knowledge Distillation\n",
    "def distillation_loss(y_true, y_pred, teacher_logits, temperature=5.0, alpha=0.5):\n",
    "    student_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    distillation_loss = tf.keras.losses.KLDivergence()(\n",
    "        tf.nn.softmax(teacher_logits / temperature),\n",
    "        tf.nn.softmax(y_pred / temperature)\n",
    "    )\n",
    "    return alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "\n",
    "# Train student model with teacher model's logits\n",
    "def train_student_model(student_model, teacher_model, X_train, y_train):\n",
    "    teacher_logits = teacher_model.predict(X_train)\n",
    "\n",
    "    # Custom training loop\n",
    "    start_kd_time = time.time()    \n",
    "    for epoch in range(10):\n",
    "        print(f\"Epoch {epoch + 1}/10\")\n",
    "        epoch_loss = 0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for i in range(0, len(X_train), 32):\n",
    "            X_batch = X_train[i:i+32]\n",
    "            y_batch = y_train[i:i+32]\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = student_model(X_batch, training=True)\n",
    "                loss = distillation_loss(y_batch, y_pred, teacher_logits[i:i+32])\n",
    "            grads = tape.gradient(loss, student_model.trainable_variables)\n",
    "            student_model.optimizer.apply_gradients(zip(grads, student_model.trainable_variables))\n",
    "\n",
    "            # Track loss and accuracy\n",
    "            epoch_loss += loss.numpy().sum() * len(y_batch)  # Ensure loss is a scalar\n",
    "            correct_predictions += np.sum(np.argmax(y_pred.numpy(), axis=-1) == np.argmax(y_batch, axis=-1))\n",
    "            total_predictions += len(y_batch)\n",
    "\n",
    "        average_loss = epoch_loss / total_predictions\n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        print(f\"Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_kd_time \n",
    "    print(f\"Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "# Train the student model\n",
    "student_model = original_model\n",
    "train_student_model(student_model, final_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNM74FvgcpeZ",
    "outputId": "ded009ee-d7bb-4512-f295-fc1719c57940"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0725 - accuracy: 0.9921\n",
      "Student Model Accuracy: 0.9921000003814697, Loss: 0.07250040769577026\n"
     ]
    }
   ],
   "source": [
    "# Evaluate student model\n",
    "student_loss, student_accuracy = student_model.evaluate(X_test, y_test)\n",
    "print(f\"Student Model Accuracy: {student_accuracy}, Loss: {student_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvNc6DjyBeIV",
    "outputId": "a7da896f-6769-4ec5-b1c8-9bd9406f5449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1473.87 KB\n"
     ]
    }
   ],
   "source": [
    "# model_save_path = 'cnn_model.h5'\n",
    "model_save_path = 'student_model2.keras'\n",
    "student_model.save(model_save_path)\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "print(f\"Model size: {model_size / 1024:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "digit-recognition",
   "language": "python",
   "name": "digit-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
