{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_TklzLYAcDe"
   },
   "source": [
    "# Import & Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a6oWnah-AcDg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JptjJAtLAcDh"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Je60zRQJAcDh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "leXZz5H-AcDi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from plotly import tools, subplots\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yraXDoo8AcDj"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\user\\anaconda3\\envs\\digit-recognition\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, img_path), cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# img = image_resize(img, height = image_size_height, width=image_size_width)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m Initial_X_data\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[0;32m     26\u001b[0m Initial_Y_data\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file\n",
    "dataset_path = \"E:/Lian/S2/Datasets/kannada_characters/\"\n",
    "# df = pd.read_csv(dataset_path+\"kannada.csv\", nrows=10)\n",
    "df = pd.read_csv(dataset_path+\"kannada.csv\", nrows=100)\n",
    "# df = pd.read_csv(dataset_path+\"kannada.csv\", nrows=10000)\n",
    "\n",
    "# Prepare image paths and labels\n",
    "image_paths = df['img'].values\n",
    "labels = df['class'].values\n",
    "\n",
    "Initial_X_data = []\n",
    "Initial_Y_data = []\n",
    "\n",
    "# image_size_width = 600\n",
    "# image_size_height = 450\n",
    "# Adjust the input size to be larger if possible\n",
    "image_size_width, image_size_height = 64, 128\n",
    "# image_size_height, image_size_width = 28, 28\n",
    "\n",
    "for img_path, label in zip(image_paths, labels):\n",
    "    img = cv2.imread(os.path.join(dataset_path, img_path), cv2.IMREAD_GRAYSCALE)\n",
    "    # img = image_resize(img, height = image_size_height, width=image_size_width)\n",
    "    img = cv2.resize(img, (image_size_height, image_size_width))\n",
    "\n",
    "    Initial_X_data.append(img)\n",
    "    Initial_Y_data.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_images = 10\n",
    "cols = 5  # Number of columns\n",
    "rows = math.ceil(num_images / cols)  # Calculate number of rows\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))  # Adjust size based on rows\n",
    "\n",
    "# Iterate over the images and axes to plot each image\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < num_images:\n",
    "        ax.imshow(Initial_X_data[i], cmap='gray')  # Display the image\n",
    "        ax.set_title(f'Label: {Initial_Y_data[i]}')  # Set the title\n",
    "    ax.axis('off')  # Hide the axis\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays and normalize pixel values\n",
    "X_data = np.array(Initial_X_data).astype('float32') / 255\n",
    "Y_data = np.array(Initial_Y_data)\n",
    "\n",
    "# Number of classes (adjust based on your specific case)\n",
    "num_classes = len(np.unique(Y_data))\n",
    "print(\">>>>>>>>>.awlawko\")\n",
    "print(num_classes)\n",
    "\n",
    "# Convert to one-hot encoding\n",
    "Y_categorical = np.eye(num_classes)[Y_data - 1]\n",
    "\n",
    "print(Y_categorical[0])\n",
    "\n",
    "print(X_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images to fit model input\n",
    "X_data_reshaped = X_data.reshape(-1, image_size_width, image_size_height, 1)\n",
    "X_data_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_categorical, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"X Training data shape: {X_train.shape}\")\n",
    "print(f\"Y Training data shape: {Y_train.shape}\")\n",
    "print(f\"X Test data shape: {X_test.shape}\")\n",
    "print(f\"Y Test data shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.expand_dims(X_train, axis=-1) / 255.0\n",
    "# X_test = np.expand_dims(X_test, axis=-1) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctm7qVa5AcDp"
   },
   "source": [
    "# Define the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    # First Convolutional + MaxPooling block\n",
    "    layers.Conv1D(filters=128, kernel_size=3, strides=1, activation='relu', input_shape=(image_size_width, image_size_height)),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2),\n",
    "\n",
    "    # Second Convolutional + MaxPooling block\n",
    "    layers.Conv1D(filters=128, kernel_size=3, strides=1, activation='relu'),\n",
    "    layers.MaxPooling1D(pool_size=2, strides=2),\n",
    "\n",
    "    # Third Convolutional block\n",
    "    layers.Conv1D(filters=128, kernel_size=3, strides=1, activation='relu'),\n",
    "\n",
    "    # Fourth Convolutional block\n",
    "    layers.Conv1D(filters=128, kernel_size=3, strides=1, activation='relu'),\n",
    "\n",
    "    # Flatten + Dense layer\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "\n",
    "    # Output Softmax layer (assuming you have `num_classes` for output neurons)\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZ2DAHnNAcDp"
   },
   "source": [
    "# Compile and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_JbhrEX6AcDq",
    "outputId": "1e936ff2-061f-49f5-b19f-aca332d3d5c7"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10)\n",
    "# history = model.fit(X_train, Y_train, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNRpUtiDAcDq"
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPjXoCCtncKR"
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxuPZ2T1nejf"
   },
   "outputs": [],
   "source": [
    "def plot_images_sample(X, Y):\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    rand_indicies = np.random.randint(len(X), size=25)\n",
    "\n",
    "    for i in range(25):\n",
    "\n",
    "        plt.subplot(5,5,i+1)\n",
    "\n",
    "        plt.xticks([])\n",
    "\n",
    "        plt.yticks([])\n",
    "\n",
    "        plt.grid(False)\n",
    "\n",
    "        index = rand_indicies[i]\n",
    "\n",
    "        plt.imshow(np.squeeze(X[index]), cmap=plt.cm.binary)\n",
    "\n",
    "        # The CIFAR labels happen to be arrays,\n",
    "\n",
    "        # which is why you need the extra index\n",
    "\n",
    "        plt.xlabel(Y[index])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# plot_images_sample(mnist_data, mnist_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3w1jut2AcDq"
   },
   "source": [
    "# Draw plot for CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "K6IYsUj_AcDr",
    "outputId": "91161044-c4f6-483b-84ee-e846869837e1"
   },
   "outputs": [],
   "source": [
    "# Draw plot for CNN training\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.ylim([0.5, 1])\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vh5jqDsnAcDr",
    "outputId": "a151681f-e942-42c2-ec3f-be2dc9d922f2"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test)\n",
    "\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SATT44wdAcDs"
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 843
    },
    "id": "rcvyrtgeAcDs",
    "outputId": "cd9fb1e2-50e8-4ff9-e652-618b27cd24ba"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "plot_images_sample(X_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cwEsINuAcDt"
   },
   "source": [
    "# Wrong test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 826
    },
    "id": "kRkz_ioxAcDt",
    "outputId": "f22f127c-c07f-4e9b-e7f8-862edbda1ce8"
   },
   "outputs": [],
   "source": [
    "X_test_wrong = []\n",
    "\n",
    "predictions_wrong = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "\n",
    "    if predictions[i] != np.argmax(Y_test[i]):\n",
    "\n",
    "        X_test_wrong.append(X_test[i])\n",
    "\n",
    "        predictions_wrong.append(predictions[i])\n",
    "\n",
    "\n",
    "plot_images_sample(X_test_wrong, predictions_wrong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KyF6_k1AcDx"
   },
   "source": [
    "# Save the model to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zIgUicAlAcDx",
    "outputId": "dc6b9409-19f9-4472-f2d1-117dfbbc5728"
   },
   "outputs": [],
   "source": [
    "# model_save_path = 'cnn_model.h5'\n",
    "\n",
    "model_save_path = 'kannada_model_1.keras'\n",
    "\n",
    "model.save(model_save_path)\n",
    "\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "\n",
    "print(f\"Model size: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q tensorflow-model-optimization\n",
    "import tensorflow_model_optimization as tfmot  # TensorFlow Model Optimization toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UNyLpBrAcDx"
   },
   "source": [
    "# Define the model and apply the sparsity API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDyZ7SdTAcD6",
    "outputId": "43b7aef7-d1c3-47ca-e912-b9d0c7b2fbe0"
   },
   "outputs": [],
   "source": [
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "pruning_params = {\n",
    "\n",
    "      # 'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100)\n",
    "\n",
    "      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.1, begin_step=0, frequency=100)\n",
    "\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "pruned_model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "\n",
    "\n",
    "# Use smaller learning rate for fine-tuning\n",
    "\n",
    "# opt = keras.optimizers.Adam(learning_rate=1e-5)\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "pruned_model.compile(\n",
    "\n",
    "  # loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  loss=keras.losses.CategoricalCrossentropy(),\n",
    "  optimizer=opt,\n",
    "\n",
    "  metrics=['accuracy']\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pruned_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P-bMkwxAcD7"
   },
   "source": [
    "# Pruning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNLmQDdxAcD7",
    "outputId": "4845c0dd-ed79-4985-d12a-6f2216830b7d"
   },
   "outputs": [],
   "source": [
    "pruned_model.fit(\n",
    "\n",
    "  X_train,\n",
    "\n",
    "  Y_train,\n",
    "\n",
    "  epochs=10,\n",
    "\n",
    "  validation_data=(X_test, Y_test),\n",
    "  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nA5jCS79I3hu",
    "outputId": "5b178c0b-4d71-4322-b00c-897c061d6c78"
   },
   "outputs": [],
   "source": [
    "model_save_path = 'kannada_model_pruned_1.keras'\n",
    "\n",
    "pruned_model.save(model_save_path)\n",
    "\n",
    "\n",
    "# Get the size of the model in bytes\n",
    "\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "\n",
    "print(f\"Model size after pruning: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2DYobRsR6J7",
    "outputId": "4d799a5a-1a0f-46db-b2c6-9d533dd2ea1a"
   },
   "outputs": [],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "\n",
    "model_save_path = 'kannada_model_pruned_1.tflite'\n",
    "\n",
    "with open(model_save_path, 'wb') as f:\n",
    "\n",
    "    f.write(tflite_model)\n",
    "\n",
    "\n",
    "\n",
    "model_size = os.path.getsize(model_save_path)\n",
    "\n",
    "print(f\"Model size after pruning + convert to tf lite: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkYL9ENMVemN"
   },
   "source": [
    "# Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EIYuob3MVhSF"
   },
   "outputs": [],
   "source": [
    "# # Strip the pruning wrappers to finalize the pruned model\n",
    "# final_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# # Convert to TensorFlow Lite model with quantization\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(final_model)\n",
    "\n",
    "# # Set quantization parameters\n",
    "# converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Default optimization includes quantization\n",
    "\n",
    "# # Optionally, if you have representative data, you can enable full integer quantization:\n",
    "# def representative_dataset():\n",
    "#     # Increase the dataset size or use a larger sample if needed, X_test.take(1000) or the full dataset\n",
    "#     for data in X_test.take(1000):  # Adjust this number as needed\n",
    "#         yield [tf.dtypes.cast(data, tf.float32)]  # Keep data type as float32 for this case\n",
    "\n",
    "# # Assign the representative dataset for integer quantization\n",
    "# converter.representative_dataset = representative_dataset\n",
    "\n",
    "# # Use float16 quantization (you can switch to int8 if needed)\n",
    "# converter.target_spec.supported_types = [tf.float16]  \n",
    "# # Uncomment this if you wish to try int8: \n",
    "# # converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "# # Convert the model\n",
    "# quantized_tflite_model = converter.convert()\n",
    "\n",
    "# # Save the quantized model to a file\n",
    "# quantized_model = 'quantized_pruned_model_kannada_1.tflite'\n",
    "# with open(quantized_model, 'wb') as f:\n",
    "#     f.write(quantized_tflite_model)\n",
    "\n",
    "# # Load and use the quantized TFLite model\n",
    "# interpreter = tf.lite.Interpreter(model_path=quantized_model)\n",
    "# interpreter.allocate_tensors()\n",
    "w\n",
    "\n",
    "# WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n",
    "# INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmpshoc9qdi\\assets\n",
    "# INFO:tensorflow:Assets written to: C:\\Users\\User\\AppData\\Local\\Temp\\tmpshoc9qdi\\assets\n",
    "# ---------------------------------------------------------------------------\n",
    "# ValueError                                Traceback (most recent call last)\n",
    "# Cell In[32], line 34\n",
    "#      32 # Load and use the quantized TFLite model\n",
    "#      33 interpreter = tf.lite.Interpreter(model_path=quantized_model)\n",
    "# ---> 34 interpreter.allocate_tensors()\n",
    "\n",
    "# File ~\\anaconda3\\envs\\digit-recognition\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:513, in Interpreter.allocate_tensors(self)\n",
    "#     511 def allocate_tensors(self):\n",
    "#     512   self._ensure_safe()\n",
    "# --> 513   return self._interpreter.AllocateTensors()\n",
    "\n",
    "# ValueError: vector too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfaA3NBHrJHh"
   },
   "outputs": [],
   "source": [
    "final_model.compile(\n",
    "\n",
    "  # loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "  loss=keras.losses.CategoricalCrossentropy(),\n",
    "\n",
    "  optimizer=opt,\n",
    "\n",
    "  metrics=['accuracy']\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h6qoUcObq2w-",
    "outputId": "645c2ab6-f34f-4328-8066-f18654024c84"
   },
   "outputs": [],
   "source": [
    "final_model.fit(\n",
    "\n",
    "  X_train,\n",
    "\n",
    "  Y_train,\n",
    "\n",
    "  epochs=10,\n",
    "\n",
    "  validation_data=(X_test, Y_test),\n",
    "    \n",
    "  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8x5jEvJ2Vrcz",
    "outputId": "7adfafec-52c5-4e59-c19e-326defd52498"
   },
   "outputs": [],
   "source": [
    "model_size = os.path.getsize(quantized_model)\n",
    "\n",
    "print(f\"Model size after quantization: {model_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aK-s0GOQY-zQ"
   },
   "source": [
    "# Optimized Model with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAu3DrpuZAG0",
    "outputId": "bb8d18db-5f92-4817-ffad-06b366e99974"
   },
   "outputs": [],
   "source": [
    "# # Custom loss function for Knowledge Distillation\n",
    "# def distillation_loss(y_true, y_pred, teacher_logits, temperature=5.0, alpha=0.5):\n",
    "#     student_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "#     distillation_loss = tf.keras.losses.KLDivergence()(\n",
    "#         tf.nn.softmax(teacher_logits / temperature),\n",
    "#         tf.nn.softmax(y_pred / temperature)\n",
    "#     )\n",
    "#     return alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "\n",
    "# # Train student model with teacher model's logits\n",
    "# def train_student_model(student_model, teacher_model, X_train, y_train):\n",
    "#     teacher_logits = teacher_model.predict(X_train)\n",
    "\n",
    "#     # Custom training loop\n",
    "#     start_kd_time = time.time()    \n",
    "#     for epoch in range(10):\n",
    "#         print(f\"Epoch {epoch + 1}/10\")\n",
    "#         epoch_loss = 0\n",
    "#         correct_predictions = 0\n",
    "#         total_predictions = 0\n",
    "        \n",
    "#         for i in range(0, len(X_train), 32):\n",
    "#             X_batch = X_train[i:i+32]\n",
    "#             y_batch = y_train[i:i+32]\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 y_pred = student_model(X_batch, training=True)\n",
    "#                 loss = distillation_loss(y_batch, y_pred, teacher_logits[i:i+32])\n",
    "#             grads = tape.gradient(loss, student_model.trainable_variables)\n",
    "#             student_model.optimizer.apply_gradients(zip(grads, student_model.trainable_variables))\n",
    "\n",
    "#             # Track loss and accuracy\n",
    "#             epoch_loss += loss.numpy().sum() * len(y_batch)  # Ensure loss is a scalar\n",
    "#             correct_predictions += np.sum(np.argmax(y_pred.numpy(), axis=-1) == np.argmax(y_batch, axis=-1))\n",
    "#             total_predictions += len(y_batch)\n",
    "\n",
    "#         average_loss = epoch_loss / total_predictions\n",
    "#         accuracy = correct_predictions / total_predictions\n",
    "#         print(f\"Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#     end_time = time.time()\n",
    "#     total_time = end_time - start_kd_time \n",
    "#     print(f\"Total time: {total_time:.2f} seconds\")\n",
    "\n",
    "# # Train the student model\n",
    "# student_model = model\n",
    "# train_student_model(model, final_model, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_VJJWrMkTI3I",
    "outputId": "4f7afe7a-34c2-445d-8a04-7c667d49a209"
   },
   "outputs": [],
   "source": [
    "# # Evaluate student model\n",
    "\n",
    "# student_loss, student_accuracy = student_model.evaluate(X_test, Y_test)\n",
    "# print(f\"Student Model Accuracy: {student_accuracy}, Loss: {student_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNVOGzHFJGAF",
    "outputId": "74e11e04-b915-4687-9195-ea53908c16ac"
   },
   "outputs": [],
   "source": [
    "# model_save_path = 'student_model_1_kannada.keras'\n",
    "# student_model.save(model_save_path)\n",
    "\n",
    "\n",
    "# # Get the size of the model in bytes\n",
    "\n",
    "# model_size = os.path.getsize(model_save_path)\n",
    "\n",
    "# print(f\"Model size: {model_size / 1024:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1175444,
     "sourceId": 1972687,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "digit-recognition",
   "language": "python",
   "name": "digit-recognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
